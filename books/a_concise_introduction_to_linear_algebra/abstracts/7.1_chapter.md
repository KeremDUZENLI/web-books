# Chapter 7.1: Eigenvalues and Eigenvectors, Basic Properties

---

## Main Idea

- This subchapter addresses the problem of matrices "mixing" the components of vectors (coupling) and introduces a method to unmix (decouple) them.

- It defines **eigenvalues** and **eigenvectors** as the specific scalars and directions that remain invariant (only scaled) under a linear transformation, allowing complex matrix operations to be simplified into independent scalar multiplications.

---

## Keywords

**Eigenvalue ($\lambda$):**

- A scalar number associated with a matrix $A$; it represents the factor by which the matrix stretches or shrinks a vector along a specific invariant direction.

**Eigenvector ($s$):**

- A non-zero vector whose direction does not change when the matrix $A$ is applied to it; $A$ acts on it simply by scaling its length.

**Characteristic Equation:**

- The algebraic equation $\det(A - \lambda I) = 0$; solving this polynomial equation yields all the eigenvalues of the matrix.

**Eigenspace:**

- The set of all eigenvectors corresponding to a specific eigenvalue $\lambda$ (plus the zero vector); geometrically, it corresponds to the null space of the matrix $(A - \lambda I)$.

**Algebraic vs. Geometric Multiplicity:**

- Algebraic multiplicity is the number of times an eigenvalue appears as a root of the characteristic equation; geometric multiplicity is the dimension of the corresponding eigenspace (number of independent eigenvectors). If the geometric is less than the algebraic, the matrix is **defective**.

---

## Formulas

**The Eigenvalue Equation**

- **Formula:** $As = \lambda s$

- **Meaning:** Applying matrix $A$ to vector $s$ is the same as multiplying $s$ by the scalar $\lambda$.

**Finding Eigenvalues (Characteristic Equation)**

- **Formula:** $\det(A - \lambda I) = 0$

- **Meaning:** This condition ensures the system $(A - \lambda I)s = 0$ has a non-trivial solution (i.e., the matrix is singular).

**Finding Eigenvectors**

- **Formula:** Solve $(A - \lambda I)s = \mathbf{0}$

- **Meaning:** Once $\lambda$ is known, find the vector $s$ in the null space of the shifted matrix.

**Properties of Symmetric Matrices**

- **Formula:** If $A^T = A$ and $\lambda_1 \neq \lambda_2$, then $s_1 \cdot s_2 = 0$.

- **Meaning:** Eigenvectors belonging to different eigenvalues of a symmetric matrix are always orthogonal (perpendicular) to each other.

---

## Practical Use

**Decoupling Systems**

- Physical systems (like connected springs or chemical reactions) often have interacting variables. Finding eigenvectors allows you to rewrite the system in a new basis where variables act independently (e.g., normal modes of vibration).

**Solving Differential Equations**

- Systems where the rate of change depends on the current state ($x' = Ax$) can be solved easily if the matrix $A$ is diagonalized using its eigenvectors, turning coupled calculus problems into simple exponential growth/decay problems.

**State Evolution**

- Predicting the long-term behavior of a system (like population growth) by analyzing the dominant eigenvalue; if $|\lambda| > 1$, the system grows; if $|\lambda| < 1$, it decays.

# Chapter 8.3: The Computation of Eigenvalues and Eigenvectors

---

## Main Idea

- This subchapter explains why the theoretical method of finding eigenvalues (solving the characteristic equation) is computationally impractical for large matrices due to the massive number of terms ($n!$) in determinants and the difficulty of solving high-degree polynomials.

- It introduces the **Power Method** and its variants (Inverse and Shifted Inverse Power Methods) as efficient, iterative algorithms to compute specific eigenvalues (dominant, smallest, or those near a specific value) and their corresponding eigenvectors directly.

---

## Keywords

**Dominant Eigenvalue:**

- The eigenvalue with the largest absolute value ($|\lambda_1| > |\lambda_j|$ for all other $j$). The Power Method specifically targets this value.

**Power Method:**

- An iterative algorithm that repeatedly multiplies an initial vector by the matrix $A$. The vector tends to align with the eigenvector of the dominant eigenvalue.

**Inverse Power Method:**

- A variation where the Power Method is applied to $A^{-1}$ (effectively solving $Ax_{i+1} = x_i$). This finds the eigenvalue closest to zero (since the largest eigenvalue of the inverse corresponds to the smallest of the original).

**Shifted Inverse Power Method:**

- A technique applying the Inverse Power Method to the matrix $A - cI$. This allows you to find the eigenvalue closest to any arbitrary number $c$, effectively letting you "hunt" for all eigenvalues if you have good initial guesses.

---

## Procedure (The Algorithm)

**Direct Power Method Strategy**

1.  **Initialize:** Choose an arbitrary starting vector $x_0$ (ideally one that has a component in the direction of the dominant eigenvector).

2.  **Iterate:** Calculate $x_{i+1} = A x_i$.

3.  **Scale:** To prevent numbers from growing too large, divide the resulting vector by its largest component (scaling).

4.  **Converge:** The scaling factor will approach the dominant eigenvalue $\lambda_1$, and the vector will approach the dominant eigenvector.

---

## Practical Use

**Efficiency for Large Systems**

- Unlike the characteristic equation, which is computationally impossible for large $n$, the Power Method relies on matrix-vector multiplication, which is very fast and easy to implement on computers.

**Finding Specific Frequencies**

- In structural engineering or quantum mechanics, often only the "lowest energy" state (smallest eigenvalue) or the "fundamental frequency" (dominant eigenvalue) matters. These methods find exactly those specific values without wasting time calculating the entire spectrum.

**Google's PageRank**

- The original algorithm for ranking web pages essentially used the Power Method to find the dominant eigenvector of the "web graph" matrix, representing the steady-state probability of a user landing on a page.

### Answer 7.3.5

**a. Gradient:**
Let $f(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} = \sum_{i,j} a_{ij} x_i x_j$.
$\frac{\partial f}{\partial x_k} = \sum_{j} a_{kj} x_j + \sum_{i} a_{ik} x_i$.
Since $A$ is symmetric ($a_{ik} = a_{ki}$), the sums are identical.
$\frac{\partial f}{\partial x_k} = 2 \sum_{j} a_{kj} x_j = 2 (A\mathbf{x})_k$.
Thus vector $\nabla f = 2(A\mathbf{x})^T$ (row vector form) or $2A\mathbf{x}$ (column vector).

**b. Lagrange Multipliers:**
Maximize $f(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}$ subject to $g(\mathbf{x}) = \mathbf{x}^T \mathbf{x} - 1 = 0$.
Condition: $\nabla f = \lambda \nabla g$.
$2A\mathbf{x} = \lambda (2\mathbf{x})$.
$A\mathbf{x} = \lambda \mathbf{x}$.
Thus, any extreme point $\mathbf{x}$ must be an eigenvector, and the value of the quadratic form at that point is $\mathbf{x}^T(\lambda \mathbf{x}) = \lambda \mathbf{x}^T \mathbf{x} = \lambda$.
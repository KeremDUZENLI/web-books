### Answer 6.2.5

**Proof:**
We want to verify $\sum x_i \mathbf{a}_i = \mathbf{b}$ where $x_i = \frac{|A_i|}{|A|}$.
Substitute $x_i$:
$$ \sum_{i=1}^n \frac{det(A_i)}{det(A)} \mathbf{a}_i = \mathbf{b} $$
Multiply by $det(A)$:
$$ \sum_{i=1}^n det(A_i) \mathbf{a}_i = det(A) \mathbf{b} $$
Consider the $k$-th component of this vector equation.
Left side: $\sum_{i=1}^n det(A_i) a_{ki}$.
$det(A_i)$ is the determinant of $A$ with column $i$ replaced by $\mathbf{b}$.
Expand $det(A_i)$ along the $i$-th column (which is $\mathbf{b}$):
$det(A_i) = \sum_{j=1}^n b_j C_{ji}$ (where $C_{ji}$ is the cofactor).
So LHS$_k$ = $\sum_{i=1}^n (\sum_{j=1}^n b_j C_{ji}) a_{ki}$.
Swap sums: $\sum_{j=1}^n b_j (\sum_{i=1}^n a_{ki} C_{ji})$.
The inner sum $\sum_{i=1}^n a_{ki} C_{ji}$ is the expansion of determinant along row $k$ ... but wait, $C_{ji}$ is cofactor of row $j$ col $i$.
This sum is the dot product of Row $k$ of $A$ with Row $j$ of Cofactor Matrix (which corresponds to Row $j$ of Adjugate?).
Property: $\sum_{i} a_{ki} C_{ji} = \delta_{kj} det(A)$. (Sum is $det(A)$ if $k=j$, else 0).
So LHS$_k$ = $\sum_{j=1}^n b_j (\delta_{kj} det(A)) = b_k det(A)$.
This matches the $k$-th component of the RHS.
Thus the equation holds.